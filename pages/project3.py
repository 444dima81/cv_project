import streamlit as st
import torch
import torch.nn as nn
import packaging.version
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from io import BytesIO


class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)


class UNet(nn.Module):
    def __init__(self, n_classes=1):
        super().__init__()
        self.down1 = DoubleConv(3, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.down2 = DoubleConv(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.bridge = DoubleConv(128, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.conv2 = DoubleConv(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.conv1 = DoubleConv(128, 64)
        self.final = nn.Conv2d(64, n_classes, 1)

    def forward(self, x):
        d1 = self.down1(x)
        p1 = self.pool1(d1)
        d2 = self.down2(p1)
        p2 = self.pool2(d2)
        bridge = self.bridge(p2)
        u2 = self.up2(bridge)
        c2 = self.conv2(torch.cat([u2, d2], dim=1))
        u1 = self.up1(c2)
        c1 = self.conv1(torch.cat([u1, d1], dim=1))
        return torch.sigmoid(self.final(c1))


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet().to(device)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ ---
torch_version = packaging.version.parse(torch.__version__)
if torch_version >= packaging.version.parse("2.6"):
    state_dict = torch.load("models/best_unet2.pth", map_location=device, weights_only=False)
else:
    state_dict = torch.load("models/best_unet2.pth", map_location=device)

model.load_state_dict(state_dict)
model.eval()

# ----------- –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è -----------
transform = A.Compose([
    A.Resize(128, 128),
    A.Normalize(),
    ToTensorV2()
])

# ----------- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit -----------
st.title("üå≤ Forest Segmentation with U-Net")
st.write("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ª–µ—Å–∞.")

uploaded_files = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=["jpg","png","jpeg"], accept_multiple_files=True)
url_input = st.text_input("–ò–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")

def load_image_from_url(url):
    import requests
    resp = requests.get(url)
    img = Image.open(BytesIO(resp.content)).convert("RGB")
    return np.array(img)

images_to_predict = []
if uploaded_files:
    for file in uploaded_files:
        img = np.array(Image.open(file).convert("RGB"))
        images_to_predict.append(img)
if url_input:
    try:
        img = load_image_from_url(url_input)
        images_to_predict.append(img)
    except:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Å—Å—ã–ª–∫–µ.")

if st.button("–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ª–µ—Å"):
    if not images_to_predict:
        st.warning("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
    for i, img_orig in enumerate(images_to_predict):
        augmented = transform(image=img_orig)
        img_tensor = augmented["image"].unsqueeze(0).to(device)

        with torch.no_grad():
            pred_mask = model(img_tensor)[0, 0].cpu().numpy()
            pred_mask_bin = (pred_mask > 0.5).astype(np.uint8)

        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_vis = img_tensor[0].permute(1, 2, 0).cpu().numpy()
        img_vis = (img_vis * std + mean)
        img_vis = np.clip(img_vis, 0, 1)

        # Overlay –∑–µ–ª—ë–Ω–æ–π –º–∞—Å–∫–∏
        overlay = img_vis.copy()
        alpha = 0.5
        overlay[pred_mask_bin == 1] = (
            (1 - alpha) * overlay[pred_mask_bin == 1] + alpha * np.array([0, 1, 0])
        )
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = pred_mask.mean()
        st.write(f"üñº –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}, —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {confidence:.2f}")

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        fig, axes = plt.subplots(1, 2, figsize=(10, 5))
        axes[0].imshow(img_vis)
        axes[0].axis("off")
        axes[0].set_title("Original Image")
        axes[1].imshow(overlay)
        axes[1].axis("off")
        axes[1].set_title("Predicted Mask Overlay")
        st.pyplot(fig)